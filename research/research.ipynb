{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pathlib\n",
    "cwd = pathlib.Path().cwd()\n",
    "sys.path.append(cwd.parent.as_posix())\n",
    "data_folder = cwd.parent.joinpath('data')\n",
    "\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "\n",
    "from dask.distributed import Client, wait\n",
    "from auxiliary import trim_memory, select_and_sort\n",
    "\n",
    "from dask_ml.decomposition import PCA\n",
    "from telecom.transformers import ColumnsCorrector\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import KFold\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-08 20:38:32,262 - distributed.diskutils - INFO - Found stale lock file and directory '/home/avagadro/projects/mega_telecom/research/dask-worker-space/worker-q38o6sjp', purging\n"
     ]
    }
   ],
   "source": [
    "client = Client(n_workers=1)\n",
    "# client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "blocksize = '200MB'\n",
    "drop_feats = ['75', '81', '85', '139', '203']\n",
    "\n",
    "# with PCA compression: 0.7776868306270386\n",
    "bound_date = '2018-11-19'\n",
    "compress_features = True\n",
    "n_components = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # baseline\n",
    "# bound_date = ''\n",
    "# compress_features = False\n",
    "\n",
    "# # partial fit: 0.7776767294528835\n",
    "# bound_date = '2018-11-19'\n",
    "# compress_features = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## load and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# read train data\n",
    "train_data = dd.read_csv(data_folder.joinpath('data_train.csv'), blocksize=blocksize).drop('Unnamed: 0', axis=1)\n",
    "\n",
    "# select required train part\n",
    "if bound_date:\n",
    "    bound_timestamp = dt.datetime.fromisoformat(bound_date).timestamp()\n",
    "    used_train_mask = client.submit(lambda df, bound: df['buy_time'] >= bound, train_data, bound_timestamp, key='get_train_data_mask')\n",
    "    # extract and sort train data\n",
    "    train_data = client.submit(select_and_sort, train_data, mask=used_train_mask, by='buy_time', key='train_data_sort')\n",
    "else:\n",
    "    train_data = client.submit(train_data.sort_values, by='buy_time', key='train_data_sort')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-08 20:45:37,772 - distributed.worker_memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 12.50 GiB -- Worker memory limit: 15.59 GiB\n",
      "2022-06-08 20:45:39,015 - distributed.worker_memory - WARNING - Worker is at 79% memory usage. Resuming worker. Process memory: 12.45 GiB -- Worker memory limit: 15.59 GiB\n",
      "2022-06-08 20:45:39,521 - distributed.worker_memory - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 12.60 GiB -- Worker memory limit: 15.59 GiB\n",
      "2022-06-08 20:45:42,327 - distributed.worker_memory - WARNING - Worker is at 42% memory usage. Resuming worker. Process memory: 6.63 GiB -- Worker memory limit: 15.59 GiB\n"
     ]
    }
   ],
   "source": [
    "# read features\n",
    "feats_csv = dd.read_csv(data_folder.joinpath('features.csv'), sep='\\t', blocksize=blocksize).drop(['Unnamed: 0', *drop_feats], axis=1)\n",
    "# feats_csv = dd.read_csv(data_folder.joinpath('compressed_features.csv'), blocksize=blocksize).drop('Unnamed: 0', axis=1)\n",
    "\n",
    "# compress if required\n",
    "if compress_features:\n",
    "    # push dask data to the cluster separately\n",
    "    features = client.scatter(feats_csv.drop(['id', 'buy_time'], axis=1))\n",
    "    headers = client.submit(feats_csv[['id', 'buy_time']].compute, key='compute_headers')\n",
    "\n",
    "    # recast to dask array with computes sizes\n",
    "    dask_array = client.submit(dd.DataFrame.to_dask_array, features, lengths=True, key='recast_to_dask_array')\n",
    "\n",
    "    # fit PCA\n",
    "    pca_model = PCA(n_components)\n",
    "    fit_pca = client.submit(pca_model.fit, dask_array, key='fit_pca_model')\n",
    "    # trim memory\n",
    "    client.run(trim_memory)\n",
    "\n",
    "    # transform features\n",
    "    transformed = client.submit(lambda df: fit_pca.result().transform(df).compute(), dask_array, key='compress_features')\n",
    "    wait(transformed)\n",
    "\n",
    "    # concat features\n",
    "    concat = client.submit(lambda df, arr: pd.concat([df.reset_index(drop=True), pd.DataFrame(arr)], axis=1), headers, transformed, key='concat_features')\n",
    "    user_feats = concat.result()\n",
    "    del features, headers, dask_array, fit_pca, transformed, concat\n",
    "else:\n",
    "    user_feats = feats_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/avagadro/projects/mega_telecom/venv/lib/python3.9/site-packages/distributed/worker.py:4708: UserWarning: Large object of size 172.14 MiB detected in task graph: \n",
      "  [              id    buy_time             0        ... s x 5 columns]]\n",
      "Consider scattering large objects ahead of time\n",
      "with client.scatter to reduce scheduler burden and \n",
      "keep data on workers\n",
      "\n",
      "    future = client.submit(func, big_data)    # bad\n",
      "\n",
      "    big_future = client.scatter(big_data)     # good\n",
      "    future = client.submit(func, big_future)  # good\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# select required user features\n",
    "unique_ids = client.submit(lambda df: df['id'].unique().compute(), train_data, key='unique_ids_compute')\n",
    "used_feats_mask = client.submit(user_feats['id'].isin, unique_ids, key='get_used_feats_mask')\n",
    "\n",
    "# extract and sort user features\n",
    "user_feats = client.submit(select_and_sort, user_feats, mask=used_feats_mask, by='buy_time', key='user_feats_sort')\n",
    "\n",
    "# compute all data (recast to pandas DataFrame)\n",
    "train_data_df = client.submit(train_data.result().compute, key='recast_train_data')\n",
    "user_feats_df = user_feats if compress_features else client.submit(user_feats.result().compute, key='recast_user_feats')\n",
    "# user_feats_df = client.submit(user_feats.result().compute, key='recast_user_feats')\n",
    "wait([train_data_df, user_feats_df])\n",
    "\n",
    "# remove no longer needed tasks from cluster\n",
    "del used_train_mask, unique_ids, used_feats_mask, train_data, user_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# merge\n",
    "merged = client.submit(pd.merge_asof, train_data_df, user_feats_df, by='id', on='buy_time', direction='nearest', key='data_merge')\n",
    "\n",
    "# split into data/target & send to cluster\n",
    "data = client.scatter(merged.result().drop('target', axis=1))\n",
    "target = client.scatter(merged.result()['target'])\n",
    "wait([data, target])\n",
    "\n",
    "# remove no longer needed tasks from cluster\n",
    "del merged, train_data_df, user_feats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tcp://127.0.0.1:33955': 1}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trim cluster memory\n",
    "client.run(trim_memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## featuring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# build featuring pipeline\n",
    "pipeline = make_pipeline(ColumnsCorrector('drop', ['id', ]), )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. f-score: 0.7777151619934244\n"
     ]
    }
   ],
   "source": [
    "n_folds = 5\n",
    "\n",
    "metrics = []\n",
    "models = []\n",
    "class_weights = dict(enumerate(compute_class_weight('balanced', classes=[0, 1], y=target.result())))\n",
    "folds = KFold(n_splits=n_folds, shuffle=True, random_state=29)\n",
    "\n",
    "for train_index, valid_index in folds.split(target.result()):\n",
    "    # push train/valid dataframes to the cluster\n",
    "    train_df = client.scatter(data.result().iloc[train_index])\n",
    "    valid_df = client.scatter(data.result().iloc[valid_index])\n",
    "    # fit and apply featuring pipeline\n",
    "    featuring = client.submit(pipeline.fit, train_df, target, key='featuring_fit')\n",
    "    X_train = client.submit(featuring.result().transform, train_df, key='train_featuring_transform')\n",
    "    X_valid = client.submit(featuring.result().transform, valid_df, key='valid_featuring_transform')\n",
    "    # exctract targets and push them to the cluster\n",
    "    y_train = client.scatter(target.result().iloc[train_index])\n",
    "    y_valid = client.scatter(target.result().iloc[valid_index])\n",
    "\n",
    "    # LGBM\n",
    "    estimator = LGBMClassifier(random_state=17,\n",
    "                               class_weight=class_weights,\n",
    "                               n_estimators=100,\n",
    "                               learning_rate=0.15,\n",
    "                               max_depth=-1,\n",
    "                               num_leaves=31,\n",
    "                               )\n",
    "    model = client.submit(estimator.fit, X_train, y_train)\n",
    "\n",
    "    # predicts & metrics\n",
    "    prediction = client.submit(lambda mdl, df: mdl.predict(df), model, X_valid, key='compute_predictions')\n",
    "    score = client.submit(f1_score, y_valid, prediction, average='macro', key='scoring')\n",
    "    # append step result\n",
    "    models.append(model.result())\n",
    "    metrics.append(score.result())\n",
    "    # remove no longer needed tasks from cluster\n",
    "    del model, featuring, train_df, valid_df, X_train, y_train, X_valid, y_valid, prediction, score\n",
    "    # trim cluster memory\n",
    "    client.run(trim_memory)\n",
    "\n",
    "print(f'Avg. f-score: {sum(metrics) / n_folds}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "PCA сжатие до 3 компонент практически не повлияло на метрику. Но метрики различаются на фоне разницы реализаций PCA в `dask` и `pyspark`. Для дальнейшей работы взяты фичи, сжатые `pyspark`.\n",
    "\n",
    "Также следует отметить, что при отсечении обучающих данных по указанной дате, распределение коммерческих предложений \"стабилизируется\" и сама дата как признак, в силу особенностей алгоритма дерева решений, не будет иметь значения на диапазоне, превышающем тот, который был на обучении. А все тестовые данные (январь 2019) выходят за этот диапазон. В связи с этим, следует исключить дату из признаков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d90a7511b44e26062c54f2bc9e753a4fcf24e14550b703ca1e7fb969fd68a2ad"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
