{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pathlib\n",
    "cwd = pathlib.Path().cwd()\n",
    "sys.path.append(cwd.parent.as_posix())\n",
    "data_folder = cwd.parent.joinpath('data')\n",
    "\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "\n",
    "from dask.distributed import Client, wait\n",
    "from auxiliary import trim_memory\n",
    "\n",
    "from telecom.transformers import ColumnsCorrector\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import KFold\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-08 20:04:18,649 - distributed.diskutils - INFO - Found stale lock file and directory '/home/avagadro/projects/mega_telecom/research/dask-worker-space/worker-cggprzyb', purging\n"
     ]
    }
   ],
   "source": [
    "# dask.config.set({'MALLOC_TRIM_THRESHOLD_': '65536'})\n",
    "client = Client(n_workers=1)\n",
    "# client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load & prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters\n",
    "blocksize = '200MB'         # 200MB per chunk\n",
    "drop_feats = ['75', '81', '85', '139', '203']\n",
    "\n",
    "# load data\n",
    "train_data = dd.read_csv(data_folder.joinpath('data_train.csv'), blocksize=blocksize).drop('Unnamed: 0', axis=1)\n",
    "user_feats = dd.read_csv(data_folder.joinpath('features.csv'), blocksize=blocksize, sep='\\t').drop(['Unnamed: 0', *drop_feats], axis=1)\n",
    "\n",
    "# select required user features\n",
    "unique_ids = client.submit(train_data['id'].unique().compute, key='unique_ids_compute')\n",
    "used_feats_mask = client.submit(user_feats['id'].isin, unique_ids, key='get_used_feats_mask')\n",
    "\n",
    "# sort all data\n",
    "train_data = client.submit(train_data.sort_values, by='buy_time', key='train_data_sort')\n",
    "user_feats = client.submit(user_feats[used_feats_mask.result()].sort_values, by='buy_time', key='user_feats_sort')\n",
    "wait([train_data, user_feats])\n",
    "\n",
    "# remove no longer needed tasks from cluster\n",
    "del unique_ids, used_feats_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute data (recast to pandas DataFrame)\n",
    "train_data_df = client.submit(train_data.result().compute, key='recast_train_data')\n",
    "user_feats_df = client.submit(user_feats.result().compute, key='recast_user_feats')\n",
    "wait([train_data_df, user_feats_df])\n",
    "\n",
    "# remove no longer needed tasks from cluster\n",
    "del train_data, user_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge\n",
    "merged = client.submit(pd.merge_asof, train_data_df, user_feats_df, by='id', on='buy_time', direction='nearest', key='data_merge')\n",
    "\n",
    "# split into data/target & send to cluster\n",
    "data = client.scatter(merged.result().drop('target', axis=1))\n",
    "target = client.scatter(merged.result()['target'])\n",
    "wait([data, target])\n",
    "\n",
    "# remove no longer needed tasks from cluster\n",
    "del merged, train_data_df, user_feats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tcp://127.0.0.1:32883': 1}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trim cluster memory\n",
    "client.run(trim_memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## featuring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build pre-featuring pipeline to apply to the entire dataset\n",
    "pipeline = make_pipeline(ColumnsCorrector('drop', ['id', ]), )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В принципе, поскольку имеющиеся фичи никак не зависят от распределения данных, то можно применить их до train/valid разделения. Но в любой момент могут добавиться фичи, зависящие от распределения, поэтому сразу реализуем и другой подход."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # apply pre-featuring pipeline\n",
    "# featuring = client.submit(pipeline.fit, data, target, key='pre_featuring_fit')\n",
    "# transformed = client.submit(featuring.result().transform, data, key='pre_featuring_transform')\n",
    "\n",
    "# # push transformed data to the cluster\n",
    "# prepared = client.scatter(transformed.result())\n",
    "# wait(prepared)\n",
    "\n",
    "# # remove no longer needed tasks from cluster\n",
    "# del data, featuring, transformed\n",
    "# data = transformed        # this for compatibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CV fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. f-score: 0.7136670695029068\n"
     ]
    }
   ],
   "source": [
    "n_folds = 5\n",
    "\n",
    "metrics = []\n",
    "models = []\n",
    "class_weights = dict(enumerate(compute_class_weight('balanced', classes=[0, 1], y=target.result())))\n",
    "folds = KFold(n_splits=n_folds, shuffle=True, random_state=29)\n",
    "\n",
    "for train_index, valid_index in folds.split(target.result()):\n",
    "    # push train/valid dataframes to the cluster\n",
    "    train_df = client.scatter(data.result().iloc[train_index])\n",
    "    valid_df = client.scatter(data.result().iloc[valid_index])\n",
    "    # fit and apply featuring pipeline\n",
    "    featuring = client.submit(pipeline.fit, train_df, target, key='featuring_fit')\n",
    "    X_train = client.submit(featuring.result().transform, train_df, key='train_featuring_transform')\n",
    "    X_valid = client.submit(featuring.result().transform, valid_df, key='valid_featuring_transform')\n",
    "    # exctract targets and push them to the cluster\n",
    "    y_train = client.scatter(target.result().iloc[train_index])\n",
    "    y_valid = client.scatter(target.result().iloc[valid_index])\n",
    "\n",
    "    # LGBM\n",
    "    estimator = LGBMClassifier(random_state=17,\n",
    "                               class_weight=class_weights,\n",
    "                               n_estimators=100,\n",
    "                               learning_rate=0.15,\n",
    "                               max_depth=-1,\n",
    "                               num_leaves=31,\n",
    "                               )\n",
    "    model = client.submit(estimator.fit, X_train, y_train)\n",
    "\n",
    "    # predicts & metrics\n",
    "    prediction = client.submit(lambda mdl, df: mdl.predict(df), model, X_valid, key='compute_predictions')\n",
    "    score = client.submit(f1_score, y_valid, prediction, average='macro', key='scoring')\n",
    "    # append step result\n",
    "    models.append(model.result())\n",
    "    metrics.append(score.result())\n",
    "    # remove no longer needed tasks from cluster\n",
    "    del model, featuring, train_df, valid_df, X_train, y_train, X_valid, y_valid, prediction, score\n",
    "    # trim cluster memory\n",
    "    client.run(trim_memory)\n",
    "\n",
    "print(f'Avg. f-score: {sum(metrics) / n_folds}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.7136670695029068"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d90a7511b44e26062c54f2bc9e753a4fcf24e14550b703ca1e7fb969fd68a2ad"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
