# Описание кейса
Пришел запрос из отдела продаж и маркетинга:<br>
Необходимо разработать алгоритм определения вероятности подключения услуги абонентом для снижения расходов на взаимодействие с нецелевыми пользователями.


## Данные
В качестве исходных данных доступна информация об отклике абонентов на предложение подключения услуг. Действие абонента отражено в бинарном виде (подключил/не подключил). Отдельным набором предоставлены нормализованные анонимизированные признаки, характеризующие профиль потребления абонента.


## Зачача и метрики
1. Предсказание вероятности подключения услуги.
2. Формирование индивидуального предложения клиенту

Скоринг осуществляется с помощью невзвешенной метрики `f1`.


# Описание модели
## Параметры окружения
jupyter python version: 3.9.13<br>
airflow version: 2.3.1 on python 3.9.13<br>
необходимый кастомный пакет: `package/dist/telecom_transformers-1.0.0-py3-none-any.whl`<br>


## Данные
`data_train.csv` - набор тренировочных данных<br>
`data_test.csv` - набор тестовых данных<br>
`features.csv` - деперсонализированные профили пользователей<br>


## Описание ноутбуков
`research/baseline.ipynb` - базовое решение, использующее все тренировочные данные и все фичи. Сопоставление фичей по ближайшей дате<br>
`research/research.ipynb` - исследование влияния PCA компрессии фичей<br>
`research/comparison.ipynb` - исследование без распределенных вычислений, использующее тренировочные данные после 19 ноября и уже сжатые фичи. Изучение вариантов сопоставления фичей, сравнение моделей, GridSearch, random state stability<br>
`research/features.ipynb` - изучение набора пользовательских профилей<br>
`research/FE.ipynb` - разработка и проверка фичей<br>
`modeling/check.ipynb` - проверка работоспособности сохраненной модели<br>
`modeling/threshold.ipynb` - определение оптимального порога вероятности для построения рекомендаций<br>

`drafts/pyspark_pca.ipynb` - черновик для сжатия фичей с помощью PySpark. Во всей работе используются фичи, сжатые именно этим способом<br>


## Структура модели
`package/` - пакет с трансформерами для модели
`modeling/model.zip` - архивированная модель, включающая весь цикл подготовки данных<br>
`modeling/predict.py` - CLI приложение для получения прогноза<br>
`modeling/predict_data_test.csv` - файл с прогнозами модели<br>

`data/parameters.conf` - глобальная конфигурация модели (см.ниже)<br>
`data/fit_params.json` - гиперпараметры, используемые для итогового обучения (обновляются при автоматическом подборе)<br>
`data/grid_params.json` - сетка гиперпараметров для автоматического подбора<br>


## Параметры глобальной конфигурации (`parameters.conf`)
> __MODEL section__<br>
`drop_features` - пользовательские признаки, отбрасываемые до обработки<br>
`bound_date` - дата разделения датасета; данные до этой даты не будут использоваться в обучении<br>
`n_folds` - количество разбиений данных при кросс-валидации<br>

> __FIT_PARAMS section__<br>
`adaptive_class_balance` - принудительно пересчитывать баланс классов, даже если он жестко указан в параметрах обучения<br>
`update_on_cv` - обновить файл параметров обучения по итогам кросс-валидации<br>

> __FIXED section__<br>
зафиксированные параметры модели (см. документацию используемого классификатора)


## airflow
Сначала нужно запустить сам airflow. Сделать это можно по 
<a href="https://airflow.apache.org/docs/apache-airflow/stable/start/">официальной инструкции</a>.

При запуске в docker необходимо дополнительно расшарить папку для передачи данных в контейнер.
Также, необходимо добавить default filesystem connection, установить java и расширение apache-airflow-providers-apache-spark.

Мой вариант сборки airflow <a href="https://github.com/j2cry/local-airflow">здесь</a>.
Перед запуском необходимо подмонтировать нужные папки проекта, иначе airflow не увидит файлов и придется их копировать. Скрипт монтирования/размонтирования лежит в репозитории по ссылке выше.

Обратите внимание, что
1. Содержимое папки `dags` должно быть доступно внутри airflow контейнера как минимум для чтения
2. Путь к файлу `dags/jobs/common.py` должен быть именно таким, иначе airflow не сможет импортировать этот модуль внутри задач
3. Файлы конфигурации модели в папке `data` должны быть доступны внутри airflow контейнера для чтения и записи
4. Метрики и параметры модели дополнительно фиксируются в логах задач airflow
