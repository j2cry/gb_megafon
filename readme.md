## Зачача
Предсказание вероятности подключения услуги


## Параметры окружения
python version: 3.9.5<br>
airflow version: 2.3.1 on python 3.9.13<br>
необходимый кастомный пакет: `package/dist/telecom_transformers-1.0.0-py3-none-any.whl`<br>


## Данные
`data_train.csv` - набор тренировочных данных<br>
`data_test.csv` - набор тестовых данных<br>
`features.csv` - деперсонализированные профили пользователей<br>


## Описание ноутбуков
`baseline.ipynb` - базовое решение, использующее все тренировочные данные и все фичи. Сопоставление фичей по ближайшей дате<br>
`research.ipynb` - исследование влияния PCA компрессии фичей<br>
`comparison.ipynb` - исследование без распределенных вычислений, использующее тренировочные данные после 19 ноября и уже сжатые фичи. Изучение вариантов сопоставления фичей, сравнение моделей, GridSearch, random state stability<br>
`features.ipynb` - изучение набора пользовательских профилей<br>
`FE.ipynb` - разработка и проверка фичей<br>
`check.ipynb` - проверка работоспособности сохраненной модели<br>
`threshold.ipynb` - определение оптимального порога вероятности для построения рекомендаций<br>

`pyspark_pca.ipynb` - черновик для сжатия фичей с помощью PySpark. Во всей работе используются фичи, сжатые именно этим способом<br>


## Структура модели
`model.zip` - архивированная модель, включающая весь цикл подготовки данных<br>
`transformers.py` - набор используемых трансформеров<br>
`predict.py` - CLI приложение для получения прогноза<br>
`predict_data_test.csv` - файл с прогнозами модели<br>

## Параметры конфигурации
`drop` - пользовательские признаки, отбрасываемые до обработки<br>
`bound_date` - дата разделения датасета; данные до этой даты не будут использоваться в обучении<br>
`n_folds` - количество разбиений данных при кросс-валидации<br>
`adaptive_class_balance` - принудительно пересчитывать баланс классов, даже если он жестко указан в параметрах обучения<br>
`update_on_cv` - обновить файл параметров обучения по итогам кросс-валидации<br>


## airflow
Сначала нужно запустить сам airflow. Сделать это можно по 
<a href="https://airflow.apache.org/docs/apache-airflow/stable/start/">официальной инструкции</a>.

При запуске в docker необходимо дополнительно расшарить папку для передачи данных в контейнер.
Также, необходимо добавить default filesystem connection, установить java и расширение apache-airflow-providers-apache-spark.

Мой вариант сборки airflow <a href="https://github.com/j2cry/local-airflow">здесь</a>.
Перед запуском необходимо подмонтировать нужные папки проекта, иначе airflow не увидит файлов и придется их копировать. Скрипт монтирования/размонтирования лежит в репозитории по ссылке выше.

Далее,
1. Содержимое папки `dags` скопировать/перенести/создать hard link в расшаренную airflow папку `dags`.
2. Тренировочные данные и файл параметров модели скопировать/перенести/создать hard link в расшаренную airflow папку `data`
3. Запустить выполнение нужного DAG

Метрики и параметры фиксируются в логах airflow.
